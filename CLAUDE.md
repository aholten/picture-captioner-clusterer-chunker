# CLAUDE.md for The Picture Captioner-Clusterer-Chunker


Behavioral guidelines to reduce common LLM coding mistakes. Merge with project-specific instructions as needed.

**Tradeoff:** These guidelines bias toward caution over speed. For trivial tasks, use judgment.

## 1. Think Before Coding

**Don't assume. Don't hide confusion. Surface tradeoffs.**

Before implementing:
- State your assumptions explicitly. If uncertain, ask.
- If multiple interpretations exist, present them - don't pick silently.
- If a simpler approach exists, say so. Push back when warranted.
- If something is unclear, stop. Name what's confusing. Ask.

## 2. Simplicity First

**Minimum code that solves the problem. Nothing speculative.**

- No features beyond what was asked.
- No abstractions for single-use code.
- No "flexibility" or "configurability" that wasn't requested.
- No error handling for impossible scenarios.
- If you write 200 lines and it could be 50, rewrite it.

Ask yourself: "Would a senior engineer say this is overcomplicated?" If yes, simplify.

## 3. Surgical Changes

**Touch only what you must. Clean up only your own mess.**

When editing existing code:
- Don't "improve" adjacent code, comments, or formatting.
- Don't refactor things that aren't broken.
- Match existing style, even if you'd do it differently.
- If you notice unrelated dead code, mention it - don't delete it.

When your changes create orphans:
- Remove imports/variables/functions that YOUR changes made unused.
- Don't remove pre-existing dead code unless asked.

The test: Every changed line should trace directly to the user's request.

## 4. Goal-Driven Execution

**Define success criteria. Loop until verified.**

Transform tasks into verifiable goals:
- "Add validation" → "Write tests for invalid inputs, then make them pass"
- "Fix the bug" → "Write a test that reproduces it, then make it pass"
- "Refactor X" → "Ensure tests pass before and after"

For multi-step tasks, state a brief plan:
```
1. [Step] → verify: [check]
2. [Step] → verify: [check]
3. [Step] → verify: [check]
```

Strong success criteria let you loop independently. Weak criteria ("make it work") require constant clarification.

---

**These guidelines are working if:** fewer unnecessary changes in diffs, fewer rewrites due to overcomplication, and clarifying questions come before implementation rather than after mistakes.

# Plan: Abstracted per-photo image captioning system called The Picture Captioner-Clusterer-Chunker

## Context
Per-photo image captions generated by a vision-language model, with the captioning backend
abstracted so the job can be run on different hardware (current RTX 3070 vs future better GPU)
or routed to cloud API providers. The job must be resumable since 44k photos takes hours.
A separate cluster report script reads the captions and produces grouped output.

---

## Architecture

Two scripts. `caption.py` runs first and produces `captions.jsonl`. `cluster_report.py` reads it.

```
caption.py  ──→  captions.jsonl  ──→  cluster_report.py  ──→  cluster_report.csv / .md
```

The `captions.jsonl` file is portable: it can be generated on a remote/better machine and copied
back before running the report.

### Directory layout

```
picture-captioner-clusterer-chunker/
├── CLAUDE.md                        ← project instructions + this plan
├── README.md                        ← setup, usage, full pipeline walkthrough
├── pyproject.toml                   ← project metadata + optional dep groups
├── config.py                        ← pydantic-settings Settings class (reads config.env)
├── journal.py                       ← Append-only journal: sole owner of captions.jsonl I/O
├── image_loader.py                  ← load_image(): HEIC, EXIF strip, RGB, resize, orientation
├── caption.py                       ← CLI orchestrator only (typer subcommands, loop logic)
├── run_loop.sh                      ← shell loop for full production run (restarts every N images)
├── cluster_report.py                ← cluster report + CSV (reads captions.jsonl via journal.py)
├── backends/
│   ├── __init__.py                  ← BACKENDS registry dict + load_backend() factory
│   ├── base.py                      ← CaptionBackend ABC, CorruptImageError
│   ├── mock.py                      ← MockBackend (no deps, instant fake captions)
│   ├── local_hf.py                  ← LocalHFBackend (torch, transformers, bitsandbytes)
│   ├── openai_api.py                ← OpenAIBackend + XAIBackend (same SDK, different base_url)
│   ├── anthropic_api.py             ← AnthropicBackend
│   └── gemini_api.py                ← GeminiBackend
├── docs/
│   ├── api_providers.md             ← pricing + privacy research per provider
│   ├── self_hosted_models.md        ← model comparison table + hardware reqs
│   ├── adding_a_backend.md          ← 10-line template + guide for new backends
│   └── caption_samples.md           ← side-by-side caption output from each backend
└── tests/
    ├── conftest.py                  ← shared fixtures (sample image, tmp paths)
    ├── test_journal.py              ← atomicity, resume, corrupt-line handling
    └── test_caption.py              ← unit tests using MockBackend
```

### Dependency management (`pyproject.toml`)

```toml
[project]
name = "picture-captioner-clusterer-chunker"
version = "0.1.0"
requires-python = ">=3.11"
dependencies = ["pillow", "pillow-heif", "tqdm", "numpy", "typer", "pydantic", "pydantic-settings", "python-json-logger"]

[project.optional-dependencies]
local  = ["torch", "transformers", "bitsandbytes", "accelerate"]
api    = ["openai", "anthropic", "google-generativeai", "tenacity"]
report = ["sentence-transformers", "umap-learn", "hdbscan"]
dev    = ["pytest"]
```

Usage:
```bash
uv run --extra local  python caption.py --backend local ...
uv run --extra api    python caption.py --backend openai ...
uv run --extra report python cluster_report.py
uv run --extra dev    pytest tests/
```

---

## New script: `caption.py`

### CLI (built with `typer`)

```
uv run --extra local python caption.py run \
    --backend local \
    --model Qwen/Qwen2.5-VL-3B-Instruct \
    --photos-dir ~/icloud_photos \
    --captions-jsonl ./captions.jsonl \
    --prompt "Describe this photo in one sentence." \
    --max-workers 1 \
    --limit 100

uv run --extra api python caption.py run \
    --backend openai --model gpt-4o-mini \
    --photos-dir ~/icloud_photos \
    --captions-jsonl ./captions.jsonl \
    --max-workers 8

# Estimate time/cost without running inference:
uv run python caption.py estimate \
    --backend openai --model gpt-4o-mini \
    --photos-dir ~/icloud_photos \
    --captions-jsonl ./captions.jsonl

# Retry only previous API errors:
uv run --extra api python caption.py run \
    --backend openai --model gpt-4o-mini \
    --retry-status error_api

# Full wipe and restart:
uv run --extra local python caption.py run --backend local --force
```

**Subcommands (typer):**
- `run` — main captioning loop (all flags above)
- `estimate` — scan remaining photos, print time/cost estimate without calling any model/API
- `stats` — print summary of an existing `captions.jsonl` (success/error counts, last run date)

**`--dry-run` flag on `run`:** Executes the full pipeline — file walk, `load_image()` (HEIC
decode, EXIF strip, RGB convert) — but replaces the backend call with the `MockBackend`. This
validates that:
- All files on disk are found and walkable
- Every image can be opened and decoded without errors
- Path → `rel_path` mapping is correct
- `captions.jsonl` is written correctly

Use before a long production run to surface any corrupt files upfront, without loading the GPU
model or spending API credits. Distinct from `estimate` (which only counts files): `--dry-run`
actually opens every image.

```bash
# Validate entire library before overnight run — catches corrupt files, wrong paths
uv run python caption.py run --backend mock --dry-run --photos-dir ~/icloud_photos
# All errors logged to caption.log and captions.jsonl (as error_corrupt records)
```

**`--restart-every N` flag (default: 0 = disabled):**
After processing N new images, `caption.py run` exits with code `2` ("batch complete, more to do").
Exit code `0` means all remaining photos are done. This allows a shell loop to restart the process
periodically, clearing GPU memory and system RAM between batches:

```bash
# run_loop.sh — provided at project root for the full production run
#!/usr/bin/env bash
set -e
BACKEND="${1:-local}"
MODEL="${2:-Qwen/Qwen2.5-VL-3B-Instruct}"
BATCH=5000

while true; do
    uv run --extra local python caption.py run \
        --backend "$BACKEND" --model "$MODEL" \
        --restart-every "$BATCH"
    EXIT=$?
    if [ $EXIT -eq 0 ]; then
        echo "All photos captioned."
        break
    elif [ $EXIT -eq 2 ]; then
        echo "Batch of $BATCH done. Restarting..."
    else
        echo "Error (exit $EXIT). Stopping."
        exit $EXIT
    fi
done
```

`run_loop.sh` is a convenience wrapper for the production run.

**`estimate` output example:**
```
Photos remaining:  43,100 / 44,583 (1,483 already done)
Backend:           openai / gpt-4o-mini
Estimated tokens:  ~61,110,000  (avg 1,418 tokens/image)
Estimated cost:    ~$9.17  (input) + ~$2.15  (output) = ~$11.32
Estimated time:    ~24 min at 8 workers
```

**Config via `pydantic-settings` (`config.py`):**
```python
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file="config.env", env_file_encoding="utf-8")
    PHOTOS_DIR: Path                # required — no default (path to photo library)
    OPENAI_API_KEY: str = ""
    ANTHROPIC_API_KEY: str = ""
    GEMINI_API_KEY: str = ""
    XAI_API_KEY: str = ""
```

All API key fields default to `""` intentionally — `Settings()` itself doesn't know which
backend will be used. Key validation is deferred to **backend `__init__`** time, giving a
specific, actionable error:

```python
class OpenAIBackend(CaptionBackend):
    def __init__(self, model: str, settings: Settings) -> None:
        if not settings.OPENAI_API_KEY:
            raise ValueError(
                "OPENAI_API_KEY is not set in config.env. "
                "Expected: OPENAI_API_KEY=sk-..."
            )
        self.client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
```

This means a typo like `OPENAI_KEY=sk-...` fails immediately at `caption.py run` startup
(before any photos are processed), not silently mid-run.
`pydantic-settings` added to core dependencies (lightweight).

### Resumability & path portability

**`captions.jsonl` schema** — one JSON object per line:
```json
{"rel_path": "2024/01/IMG_1234.JPG", "caption": "a family at a dinner table", "status": "success"}
{"rel_path": "2024/01/IMG_5678.HEIC", "caption": null, "status": "error_corrupt"}
{"rel_path": "2024/01/IMG_9999.JPG", "caption": null, "status": "error_api"}
```

| Field | Type | Description |
|---|---|---|
| `rel_path` | string | Path relative to `--photos-dir` |
| `caption` | string \| null | Generated caption, or `null` on failure |
| `status` | string | `success`, `error_corrupt`, `error_api`, `error_model` |

**Status meanings:**
- `success` — caption generated; `caption` field is populated
- `error_corrupt` — image could not be opened/decoded (PIL failure, truncated HEIC, etc.)
- `error_api` — API call failed (auth, rate limit, network timeout)
- `error_model` — local model inference threw an exception

**Error handling — never crash on bad media:**
The outer loop wraps each photo in a strict try/except. Any failure writes an error record to
`captions.jsonl`, appends a line to `errors.log`, and calls `continue` to keep the batch moving:

```python
try:
    image = load_image(full_path)          # EXIF stripped, RGB, validated
    caption = backend.caption(image)       # backend receives clean PIL.Image
    journal.write(rel_path, caption, "success")
except CorruptImageError as e:
    journal.write(rel_path, None, "error_corrupt")
    logger.warning("error_corrupt", extra={"rel_path": rel_path, "error": str(e)})
except APIError as e:
    journal.write(rel_path, None, "error_api")
    logger.warning("error_api", extra={"rel_path": rel_path, "error": str(e)})
except Exception as e:
    journal.write(rel_path, None, "error_model")
    logger.warning("error_model", extra={"rel_path": rel_path, "error": str(e)})
```

**Structured logging via Python `logging` + `python-json-logger`:**
All output goes through a configured logger (not `print()`), with two handlers:
- Console handler: human-readable (level INFO+)
- File handler: `caption.log` — newline-delimited JSON, one event per line, for later analysis

```json
{"asctime": "2025-02-26T02:14:33", "levelname": "WARNING", "event": "error_corrupt",
 "rel_path": "2022/IMG_5678.HEIC", "error": "PIL UnidentifiedImageError: cannot identify image file"}
```

This replaces the `errors.log` file — all events (progress, warnings, errors) are in `caption.log`.
`python-json-logger` added to core dependencies.

**Pydantic models for JSONL records (`journal.py`):**
```python
from pydantic import BaseModel
from typing import Literal

class CaptionRecord(BaseModel):
    rel_path: str
    caption: str | None
    status: Literal["success", "error_corrupt", "error_api", "error_model"]
```
`CaptionJournal.load()` uses `CaptionRecord.model_validate_json(line)` per line —
`ValidationError` is caught and logged, never crashes the loader. `pydantic` added to core deps.

**Progress bar — total count known upfront:**
On startup, before any inference, `caption.py` performs one `os.walk` pass to collect all image
file paths (filtered by extension). Total is set on the `tqdm` bar before the first image is
processed, giving accurate ETA from the start.

Video files (`.mp4`, `.mov`) that the embeddings pipeline included will hit `error_corrupt`.

**Concurrency — `--max-workers N`:**
Single-threaded is correct for local GPU (the model itself batches internally). For API backends,
network round-trips dominate — parallel requests are 5–10× faster.

```
--max-workers 1     # default; required for local backend (GPU memory constraint)
--max-workers 8     # recommended starting point for OpenAI / Anthropic
--max-workers 20    # Gemini handles higher concurrency
```

Implementation: `concurrent.futures.ThreadPoolExecutor` with a `threading.Semaphore` on top
to honour `--max-workers`. Tenacity decorators still work per-thread.

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from itertools import islice

def _batched(iterable, n):
    it = iter(iterable)
    while chunk := list(islice(it, n)):
        yield chunk

SUBMIT_BATCH = 500   # bound in-flight futures; avoids 44k Future objects in memory at once

# Build work list once before any workers start.
# is_done() checked at scheduling time — all journal reads complete, no writes yet.
pending = [(rp, fp) for rp, fp in all_photos if not journal.is_done(rp)]

with ThreadPoolExecutor(max_workers=args.max_workers) as pool:
    for batch in _batched(pending, SUBMIT_BATCH):
        futures = {pool.submit(process_one, rp, fp): rp for rp, fp in batch}
        for future in as_completed(futures):
            future.result()   # process_one catches exceptions internally; re-raises only unexpected ones
```

**Thread-safety design note:** `is_done()` is intentionally checked at scheduling time (before
the executor starts), not inside the worker. The journal is fully loaded before any writes happen,
so the snapshot is consistent for the entire run. **Known constraint:** running two `caption.py`
processes against the same `captions.jsonl` simultaneously is unsupported — single-writer
assumption is explicit and documented.

`CaptionJournal.write()` acquires a `threading.Lock` before appending + fsyncing, ensuring
no interleaved partial lines from concurrent workers within the same process.

Local backend: `--max-workers` is silently clamped to 1 with a notice:
`"Local backend is single-threaded (GPU memory constraint). Ignoring --max-workers."`

**API rate limiting — exponential backoff via `tenacity`:**
All API backends use `tenacity` decorators so retry logic is declarative and consistent.
`tenacity` is added to the `api` optional-dependency group:

```python
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    retry=retry_if_exception_type((RateLimitError, APIConnectionError)),
    wait=wait_exponential(multiplier=1, min=1, max=60),
    stop=stop_after_attempt(5),
    reraise=True,   # raise the final exception so the outer loop catches it as error_api
)
def _call_api(self, image_bytes: bytes) -> str:
    return self.client.generate(...)
```

- Retries on HTTP 429 (rate limit) and 502/503/network errors (transient)
- Does NOT retry on 400/401/403 (auth/bad request — permanent failures)
- After 5 attempts, `reraise=True` causes the outer loop to catch it as `error_api` and move on
- `tenacity` added to `[project.optional-dependencies] api` group in `pyproject.toml`

**Write strategy — atomic per-record:**
Each record is written as a single complete JSON line, immediately followed by `file.flush()` +
`os.fsync(file.fileno())`. This ensures the record hits disk before the next image is processed.
A hard kill (SSH drop, power loss, SIGKILL) can at most corrupt the record currently being written
— it cannot corrupt previously written records.

**Resume behaviour:**
- On startup: read `captions.jsonl` line-by-line, parsing each line as JSON
- Any line that fails to parse (truncated by hard kill) is silently discarded with a warning:
  `"Warning: discarded 1 malformed line from captions.jsonl"`
- Build the skip-set from all successfully parsed records; which statuses to skip is controlled
  by CLI flags (see below)
- Console on resume: `"Skipping 1240 already processed (1198 success, 42 errors)"`

**Resume CLI flags:**

| Flag | Behaviour |
|---|---|
| *(default)* | Skip everything in `captions.jsonl` (any status) — safest |
| `--retry-status error_api,error_model` | Re-attempt records with these statuses |
| `--force` | Ignore `captions.jsonl` entirely; reprocess all photos from scratch |

`--retry-status` removes matching records from the in-memory skip-set before the run starts
(does not modify the file). New results for those paths are appended; duplicates are resolved
at read time by `CaptionJournal.load()` which takes the **last** record per `rel_path`
(later entries win). This avoids rewriting the whole file for a retry.

`error_corrupt` is never included in any `--retry-status` default suggestion, since corrupt
files are permanent failures that won't change between runs.

**Portability:** `rel_path` is relative to `--photos-dir` (defaults to `PHOTOS_DIR` from
`config.env`). Generate captions on a powerful remote machine, copy `captions.jsonl` back, run
`cluster_report.py` with the same `--photos-dir`.

To restart from scratch: `rm captions.jsonl`

### Caption prompt (default)
```
Describe this photo in one or two sentences. Focus on the main subject, setting, and activity.
```

### Image preprocessing layer (`image_loader.py`)

A dedicated module sits between the file system and all backends. Backends **never** receive
raw file paths — only preprocessed PIL images. Future additions (resize, TIFF support,
orientation correction from EXIF) all go here, with no changes to any backend or `caption.py`.

```python
def load_image(path: Path) -> PIL.Image.Image:
    """
    Open image, convert HEIC, strip EXIF, return RGB PIL.Image.
    Raises CorruptImageError on any I/O or decode failure.
    """
    try:
        if path.suffix.lower() in (".heic", ".heif"):
            from pillow_heif import register_heif_opener
            register_heif_opener()
        img = Image.open(path)
        img.load()                          # force full decode
        img = img.convert("RGB")            # normalize color space
        data = list(img.getdata())          # strip EXIF by rebuilding image
        clean = Image.new(img.mode, img.size)
        clean.putdata(data)
        return clean
    except Exception as e:
        raise CorruptImageError(path, e) from e
```

**Why this matters for API backends:** rebuilding the image via `getdata()` + `putdata()` drops
all metadata (EXIF GPS coordinates, device serial numbers, etc.) before bytes leave the machine.
The backend receives a clean RGB image with no embedded metadata.

API backends encode the PIL image to JPEG bytes in-memory (`io.BytesIO`) before sending.

### Backend abstraction (`backends/` package)

`backends/base.py` defines the shared contract:
```python
class CaptionBackend(ABC):
    def caption(self, image: PIL.Image.Image) -> str: ...
```

`backends/__init__.py` is the registry — `caption.py` never imports backends directly:
```python
from .mock import MockBackend
# Other backends imported lazily inside load_backend() to avoid import errors
# when their optional deps aren't installed

BACKENDS = {"mock": MockBackend}   # always available

def load_backend(name: str, model: str, settings) -> CaptionBackend:
    if name == "local":
        from .local_hf import LocalHFBackend
        return LocalHFBackend(model, settings)
    elif name == "openai":
        from .openai_api import OpenAIBackend
        return OpenAIBackend(model, settings)
    # ... etc.
    raise ValueError(f"Unknown backend: {name!r}. Available: {list(BACKENDS)}")
```

| Module | Backend class | `--backend` | Extra deps |
|---|---|---|---|
| `mock.py` | `MockBackend` | `mock` | None |
| `local_hf.py` | `LocalHFBackend` | `local` | torch, transformers, bitsandbytes, accelerate |
| `gemini_api.py` | `GeminiBackend` | `gemini` | google-generativeai |
| `openai_api.py` | `OpenAIBackend` | `openai` | openai, tenacity |
| `openai_api.py` | `XAIBackend` | `xai` | openai, tenacity (different base_url) |
| `anthropic_api.py` | `AnthropicBackend` | `anthropic` | anthropic, tenacity |

Each backend's `__init__` does lazy imports and raises a clear `ImportError` with install
instructions if optional deps are missing. Adding a new backend = add one file, register
one line in `load_backend()`. `caption.py` needs no changes.

**`MockBackend`** returns `{"caption": "a mock caption for {filename}", "status": "success"}` —
enables testing all plumbing (path handling, resume, Ctrl+C, progress bar, JSONL schema,
error fallback in cluster_report) without any model or API key. Accepts an optional
`--mock-error-rate 0.1` flag to simulate 10% error_corrupt failures for testing error paths.

API keys read from `config.env`:
- `GEMINI_API_KEY`
- `OPENAI_API_KEY`
- `ANTHROPIC_API_KEY`
- `XAI_API_KEY`

---

## Self-hosted model options

| Model | Released | FP16 VRAM | INT4 VRAM | RTX 3070 (8GB)? | Speed est. | HF ID |
|---|---|---|---|---|---|---|
| **Qwen2.5-VL-3B** | Jan 2025 | 7-8 GB | **3-4 GB** | ✅ int4 | ~1-2 img/s | `Qwen/Qwen2.5-VL-3B-Instruct` |
| **Qwen2.5-VL-7B** | Jan 2025 | 17 GB | **6-8 GB** | ⚠️ tight | ~0.5-1 img/s | `Qwen/Qwen2.5-VL-7B-Instruct` |
| Moondream2 | Jun 2025 | 5-10 GB | 4 GB | ✅ | ~1-2 img/s | `vikhyatk/moondream2` |
| Florence-2-large | Jun 2024 | 1.2-2 GB | <1 GB | ✅ | **5-10 img/s** | `microsoft/Florence-2-large` |
| SmolVLM2-2.2B | Feb 2025 | ~4 GB | <1 GB | ✅ | **5-20 img/s** | `HuggingFaceTB/SmolVLM2-2.2B-Instruct` |
| InternVL2-8B | Jul 2024 | 16 GB | 5-6 GB | ⚠️ marginal | ~1 img/s | `OpenGVLab/InternVL2-8B` |

**Recommended for RTX 3070 (current):** `Qwen/Qwen2.5-VL-3B-Instruct` int4 — best quality/VRAM tradeoff.
44k photos at ~1-2 img/s ≈ 6–12 hours. Run overnight, resumable.

**Recommended for better hardware (RTX 3090/4090):** `Qwen/Qwen2.5-VL-7B-Instruct` fp16 (24GB GPU)
or int4 (12GB+).

---

## API provider options

| Provider | Model | Est. cost / 44k photos | Data retention (API) | Safe for family photos? |
|---|---|---|---|---|
| **Google Gemini** | gemini-1.5-flash | ~$3–6 | 3 years default; **ZDR available** | ✅ Enable ZDR |
| **Google Gemini** | gemini-2.0-flash | ~$6–10 | 3 years default; **ZDR available** | ✅ Enable ZDR |
| **OpenAI** | gpt-4o-mini | ~$6 | 30 days; **ZDR available** | ✅ API + ZDR |
| **OpenAI** | gpt-4o | ~$19 | 30 days; **ZDR available** | ✅ API + ZDR |
| **Anthropic** | claude-haiku-4-5 | ~$70 | **Never trained on by default** | ✅ Safest |
| **xAI** | grok-2-vision-1212 | ~$44–132 | Private chats not trained | ✅ private mode |

⚠️ **Privacy warning**: All API backends upload raw image bytes to external servers.
- **Safest**: Anthropic API — data never used for training by default; ZDR addendum available.
- **Cheapest + safe**: OpenAI `gpt-4o-mini` API with Zero Data Retention setting (~$6 total).
- **Avoid**: ChatGPT web UI, Gemini without ZDR enabled.
- Always use the **API**, never the web interface, for personal photos.

---

## Changes to `cluster_report.py`

1. On startup: check for `captions.jsonl`; if found, load into dict `{rel_path: record}`
   where `record = {"caption": ..., "status": ...}`
2. **CSV label column logic per photo:**
   - `status == "success"` → use `caption`
   - `status` starts with `"error_"` → write `null` (or empty string) in label column; log warning
   - Not in `captions.jsonl` → fall back to CLIP vocabulary match
3. **Markdown cluster label**: use caption of photo nearest to centroid if that photo has
   `status == "success"`; otherwise fall back to CLIP vocabulary match
4. Report summary line: `"N captioned, M errors, K vocabulary fallbacks"`
5. VOCABULARY list and CLIP text encoding retained as fallback path — skipped entirely only
   if all photos have `status == "success"` in `captions.jsonl`
6. Existing markdown structure, UMAP/HDBSCAN, cosine distances — all unchanged

---

## Files to create

| File | Description |
|---|---|
| `README.md` | Setup guide, usage, full pipeline walkthrough |
| `pyproject.toml` | Project metadata + optional dep groups |
| `config.py` | `pydantic-settings` Settings — validates config.env |
| `journal.py` | **Append-only journal** — sole owner of captions.jsonl I/O |
| `caption.py` | CLI orchestrator only — typer subcommands, loop logic |
| `run_loop.sh` | Shell loop for production run — restarts every N images to clear GPU/RAM |
| `cluster_report.py` | Cluster report + CSV, reads captions.jsonl via `journal.py` |
| `image_loader.py` | `load_image()` — HEIC, EXIF strip, RGB, resize, orientation |
| `backends/__init__.py` | `BACKENDS` registry dict + `load_backend()` factory |
| `backends/base.py` | `CaptionBackend` ABC, `CorruptImageError` |
| `backends/mock.py` | `MockBackend` — no deps, instant fake captions |
| `backends/local_hf.py` | `LocalHFBackend` — torch, transformers, bitsandbytes |
| `backends/openai_api.py` | `OpenAIBackend` + `XAIBackend` |
| `backends/anthropic_api.py` | `AnthropicBackend` |
| `backends/gemini_api.py` | `GeminiBackend` |
| `docs/api_providers.md` | API provider pricing + privacy policy reference |
| `docs/self_hosted_models.md` | Self-hosted model comparison + hardware requirements |
| `docs/adding_a_backend.md` | 10-line template + step-by-step guide for new backends |
| `docs/caption_samples.md` | Sample captions from each backend on the same 5 test photos |
| `tests/conftest.py` | Shared test fixtures |
| `tests/test_caption.py` | Unit tests using MockBackend |
| `tests/test_journal.py` | Unit tests for journal: atomicity, resume, corrupt-line handling |

---

## Append-only journal (`journal.py`)

`captions.jsonl` is the system's source of truth — an append-only event ledger of captioning
outcomes. `journal.py` is the **only** module that reads or writes this file. All other code
calls its API.

```python
class CaptionJournal:
    def __init__(self, path: Path) -> None: ...

    def load(self, retry_statuses: set[str] = frozenset()) -> dict[str, dict]:
        """
        Read all valid records from captions.jsonl.
        Returns {rel_path: {"caption": ..., "status": ...}}.
        Silently discards lines that fail JSON parsing (truncated by hard kill).
        Last record per rel_path wins (supports --retry-status append semantics).
        Paths with status in retry_statuses are excluded from the returned dict
        (will be re-processed this run).
        """

    def is_done(self, rel_path: str) -> bool:
        """True if rel_path already has a record (any status). O(1) set lookup."""

    def write(self, rel_path: str, caption: str | None, status: str) -> None:
        """
        Append one JSON record as a complete line, then flush() + fsync().
        Updates the in-memory set so is_done() returns True immediately after.
        Thread-safe: acquires a threading.Lock before write + fsync (for --max-workers > 1).
        """

    def summary(self) -> str:
        """Returns e.g. 'Skipping 1240 already processed (1198 success, 42 errors)'"""
```

`caption.py` usage:
```python
journal = CaptionJournal(CAPTIONS_FILE)
journal.load()
print(journal.summary())

for rel_path, full_path in all_photos:
    if journal.is_done(rel_path):
        continue
    try:
        caption = backend.caption(full_path)
        journal.write(rel_path, caption, "success")
    except CorruptImageError:
        journal.write(rel_path, None, "error_corrupt")
    ...
```

---

## Documentation contents

### `docs/adding_a_backend.md`
Explains the `CaptionBackend` ABC and provides a minimal template:
```python
# my_backend.py — drop this into caption.py and register in BACKENDS dict
from caption import CaptionBackend
from PIL.Image import Image

class MyBackend(CaptionBackend):
    def __init__(self, model: str, settings) -> None:
        import my_sdk                    # lazy import — missing dep gives clear error
        self.client = my_sdk.Client(api_key=settings.MY_API_KEY)
        self.model = model

    def caption(self, image: Image) -> str:
        import io, base64
        buf = io.BytesIO()
        image.save(buf, format="JPEG")
        b64 = base64.b64encode(buf.getvalue()).decode()
        return self.client.generate(model=self.model, image_b64=b64, prompt=self.prompt)
```
Steps: (1) create `backends/my_backend.py` implementing `CaptionBackend`,
(2) register in `backends/__init__.py` `load_backend()`,
(3) add API key field to `config.py`, (4) add SDK to `api` optional dep in `pyproject.toml`,
(5) add a row to `docs/api_providers.md`. `caption.py` requires zero changes.

### `docs/caption_samples.md`
A table of 5 representative test photos (nature, group portrait, food, screenshot, pet) with the
caption produced by each backend side-by-side, so users can compare style and quality before
committing to a backend for the full 44k run.

---

## Testing plan

### Phase 0 — Unit tests (no photos needed, no GPU)
```bash
uv run --extra dev pytest tests/ -v
```
**Check:** all tests pass — MockBackend, path handling, JSONL read/write, resume logic

### Phase 1 — Mock: plumbing correctness (no GPU, no API key, no model download)
```bash
uv run --extra api python caption.py --backend mock --limit 20 --photos-dir ~/icloud_photos
```
**Check:**
- `captions.jsonl` created with 20 lines
- Each line parses as valid JSON: `{"rel_path": "...", "caption": "a mock caption for ..."}`
- `rel_path` is relative (e.g. `2022/IMG_1234.JPG`), not absolute
- Progress bar displayed with count + estimated time

### Phase 2 — Mock: resume / interrupt correctness (no GPU needed)
```bash
# Run 30 photos, Ctrl+C after ~10
uv run python caption.py --backend mock --limit 30 --photos-dir ~/icloud_photos
# (Ctrl+C)

# Check partial state
python -c "import json; lines=[json.loads(l) for l in open('captions.jsonl')]; print(len(lines))"
# → ~10 lines

# Resume — must NOT re-caption already done photos
uv run python caption.py --backend mock --limit 30 --photos-dir ~/icloud_photos
# Console should show: "Skipping N already captioned photos"
python -c "import json; lines=[json.loads(l) for l in open('captions.jsonl')]; assert len(set(l['rel_path'] for l in lines))==len(lines), 'DUPLICATES'; print('OK —', len(lines), 'rows, no duplicates')"
```

### Phase 3 — Mock: cluster_report.py integration
```bash
# With mock captions.jsonl (30 photos), run report
uv run --extra report python cluster_report.py
```
**Check:**
- `cluster_export.csv`: captioned photos use mock captions; uncaptioned fall back to vocabulary
- `cluster_report.md`: cluster label uses caption of nearest-to-centroid photo where available
- No crash when captions are only partial

### Phase 4 — Real model smoke test: local backend (RTX 3070)
```bash
# captions.jsonl from Phase 1–3 mock run is still present — that's fine.
# The 10 new real captions will be appended alongside the mock ones.
uv run --extra local python caption.py run --backend local --model Qwen/Qwen2.5-VL-3B-Instruct --limit 10 --photos-dir ~/icloud_photos
```
**Check:**
- No CUDA OOM (confirms INT4 fits in 8GB)
- Speed ~1-2 img/s in progress bar
- 10 real descriptive captions in `captions.jsonl`

### Phase 5 — Real API smoke test (optional, ~$0.005)
```bash
# Uploads 5 images to OpenAI — confirm you're comfortable with this
uv run --extra api python caption.py --backend openai --model gpt-4o-mini --limit 5 --photos-dir ~/icloud_photos
```
**Check:** Requires `OPENAI_API_KEY` in `config.env`; 5 captions appended; richer than local output

### Phase 6 — Full production run
```bash
# DO NOT delete captions.jsonl. The WAL is additive — smoke test captions from Phases 4 & 5
# are valid records. caption.py skips them automatically and continues from where they left off.

bash run_loop.sh local Qwen/Qwen2.5-VL-3B-Instruct
# Restarts every 5,000 images to clear GPU/RAM; runs until all 44,583 photos are done.
# Safe to interrupt at any time — re-run the same command to resume.
```
**When complete:**
```bash
python -c "
import json
lines = [json.loads(l) for l in open('captions.jsonl')]
paths = [l['rel_path'] for l in lines]
assert len(paths) == len(set(paths)), 'DUPLICATES'
print(f'{len(lines)} captions, no duplicates')
"
# → 44583 captions, no duplicates

uv run --extra report python cluster_report.py
# cluster_export.csv: 44583 rows + header, labels are real captions
# cluster_report.md: cluster headers show real photo captions
```
